{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot Vietnamese.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalihidroxit/NLPApp/blob/master/Chatbot_Vietnamese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui08ZvTy9gZ_",
        "colab_type": "code",
        "outputId": "db062470-3760-4004-bd28-9a3240ef9500",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import zipfile\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTMF5-4L9mNW",
        "colab_type": "code",
        "outputId": "66bce11f-8638-4fc1-c170-96ddcd41b059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLZr-uWoL7Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_path = \"/content/gdrive/My Drive/chatbot/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2-q5Zv-nLny",
        "colab_type": "code",
        "outputId": "b44ded06-d9b9-45c4-d96a-f296bc0b5780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(os.listdir(base_path))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['vocabulary.txt', 'InsuranceQAquestionanslabelraw.encoded', 'InsuranceQAlabel2answerraw.encoded', 'cornell-moviedialog-corpus.zip', 'cornell-moviedialog-corpus', 'data.txt', 'model_attention.h5']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42ahf3x3Lozg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNekf2v2T3cX",
        "colab_type": "text"
      },
      "source": [
        "# **Load the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NI2CQAgT3-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = open(base_path + 'data.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "# conv_lines = open(base_path + 'movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THXEtg7kUb5f",
        "colab_type": "text"
      },
      "source": [
        "# **Create a list of all of the conversations' lines' ids.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m519tIfN7vMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "convs = []\n",
        "for line in lines[:-1]:\n",
        "    _line = line.split('\\t')\n",
        "    convs.append(_line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XDPyaA0Uj5u",
        "colab_type": "text"
      },
      "source": [
        "#**id and conversation sample**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl3bQR6fUkMD",
        "colab_type": "code",
        "outputId": "33d2d9c9-ac76-4dd1-e34c-28ab204fbf4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "source": [
        "for k in convs[0]:\n",
        "    print (k)#, id2line[k])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xin chào.\n",
            "Chào bạn.\n",
            "Bạn khỏe chứ?\n",
            "Tôi khỏe. Bạn cũng khỏe phải không?\n",
            "Tôi khỏe.Bạn biết nói Tiếng Anh chứ?\n",
            "Một ít. Bạn là người Mỹ à?\n",
            "Vâng!\n",
            "Bạn đến từ đâu?\n",
            "Tôi đến từ California\n",
            "Hân hạnh được gặp bạn.\n",
            "Tôi cũng vậy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQ5XNfRUqnf",
        "colab_type": "text"
      },
      "source": [
        "#Sort the sentences into questions (inputs) and answers (targets)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwxUZ0qpUuuv",
        "colab_type": "code",
        "outputId": "7ce9df50-e50e-4b14-f847-ff2a541cf5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "questions = []\n",
        "answers = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(conv[i])\n",
        "        answers.append(conv[i+1])\n",
        "        \n",
        "# Compare lengths of questions and answers\n",
        "print(len(questions))\n",
        "print(len(answers))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3858\n",
            "3858\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iluqC-66Am1",
        "colab_type": "code",
        "outputId": "29c201f4-54f7-49f3-e353-41463e08fd41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(questions[0])\n",
        "print(answers[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xin chào.\n",
            "Chào bạn.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA2CZx105qT6",
        "colab_type": "code",
        "outputId": "fcd84002-2c41-4ec9-e91c-df41e7ddc96d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(questions[3])\n",
        "print(answers[3])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tôi khỏe. Bạn cũng khỏe phải không?\n",
            "Tôi khỏe.Bạn biết nói Tiếng Anh chứ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KecqDi8VLB9",
        "colab_type": "text"
      },
      "source": [
        "# Find the length of sentences (not using nltk due to processing speed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FruqjWwBVL95",
        "colab_type": "code",
        "outputId": "dedd6742-2fda-4ba6-d41f-468908a231e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "lengths = []\n",
        "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions]) #nltk approach\n",
        "for question in questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Create a dataframe so that the values can be inspected\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13.0\n",
            "16.0\n",
            "18.0\n",
            "22.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624-2C7cVcZL",
        "colab_type": "text"
      },
      "source": [
        "# Randomly check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQDmtRiLVch_",
        "colab_type": "code",
        "outputId": "c51a258f-9ae9-42dc-969f-026535e4db22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "r = np.random.randint(1,len(questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(questions[i])\n",
        "    print(answers[i])\n",
        "    print()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Chà, ngày mai tôi phải phẫu thuật trên ngón tay.\n",
            "Ngón tay bạn bị sao à?\n",
            "\n",
            "Ngón tay bạn bị sao à?\n",
            "Tôi bị gãy khi chơi bóng rổ\n",
            "\n",
            "Tôi bị gãy khi chơi bóng rổ\n",
            "Thật là tồi tệ\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txhViaGwVqb4",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing for word based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "086Io1GlVsMK",
        "colab_type": "code",
        "outputId": "c55b401e-6ed0-4cdc-b3cb-c70590c711cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "num_samples = 100000  # Number of samples to train on.\n",
        "questions = questions[:num_samples]\n",
        "answers = answers[:num_samples]\n",
        "#tokenizing the qns and answers\n",
        "questions_tok = [nltk.word_tokenize(sent) for sent in questions]\n",
        "answers_tok = [nltk.word_tokenize(sent) for sent in answers]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mdkc_OUV9Zr",
        "colab_type": "text"
      },
      "source": [
        "#train-validation split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppuE5uqJV-YL",
        "colab_type": "code",
        "outputId": "cd29217f-1bf4-473e-f329-bb9ad13256a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "data_size = len(questions_tok)\n",
        "\n",
        "# We will use the first 0-80th %-tile (80%) of data for the training\n",
        "training_input  = questions_tok[:round(data_size*(80/100))]\n",
        "training_input  = [tr_input[::-1] for tr_input in training_input] #reverseing input seq for better performance\n",
        "training_output = answers_tok[:round(data_size*(80/100))]\n",
        "\n",
        "# We will use the remaining for validation\n",
        "validation_input = questions_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #reverseing input seq for better performance\n",
        "validation_output = answers_tok[round(data_size*(80/100)):]\n",
        "\n",
        "training_size = len(training_input)\n",
        "validation_size = len(validation_input)\n",
        "print('training size', training_size)\n",
        "print('validation size', validation_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training size 3086\n",
            "validation size 772\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poz1RRB_WFwO",
        "colab_type": "text"
      },
      "source": [
        "# Word en/decoding dictionaries\n",
        "Create a dictionary for the frequency of the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQILi27SWWew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = {}\n",
        "for question in questions_tok:\n",
        "    for word in question:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "for answer in answers_tok:\n",
        "    for word in answer:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Oi4PFpK5wiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b7b24ca-78c8-4ec0-a1e8-3406a1ff3a2b"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2315"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eMLxApRWbdn",
        "colab_type": "text"
      },
      "source": [
        "# Remove rare words from the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIIC6fKBWe7i",
        "colab_type": "code",
        "outputId": "6b5bab68-0869-4aea-b41f-57fe3f9f1144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "threshold = 5\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1\n",
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of total vocab: 2315\n",
            "Size of vocab we will use: 1216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXHOALVWrIR",
        "colab_type": "text"
      },
      "source": [
        "#we will create dictionaries to provide a unique integer for each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOM0Osk4Wqht",
        "colab_type": "code",
        "outputId": "f67ac9a0-9663-4fbe-922e-51c18319f872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "WORD_CODE_START = 1\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "\n",
        "word_num  = 2 #number 1 is left for WORD_CODE_START for model decoder later\n",
        "encoding = {}\n",
        "decoding = {1: '<STA>'}\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold: #get vocabularies that appear above threshold count\n",
        "        encoding[word] = word_num \n",
        "        decoding[word_num ] = word\n",
        "        word_num += 1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of vocab used: 1218\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3aYkkjDW23c",
        "colab_type": "code",
        "outputId": "f53fe6d2-48b6-4a8e-e9fc-e90c839c0cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#include unknown token for words not in dictionary\n",
        "decoding[word_num] = '<UNK>'\n",
        "encoding['<UNK>'] = word_num\n",
        "dict_size = word_num+1\n",
        "dict_size"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhDuN9lgXAOp",
        "colab_type": "text"
      },
      "source": [
        "# Vectorizing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulcPrhl6W-Ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(encoding, data, vector_size=20):\n",
        "    \"\"\"\n",
        "    :param encoding: encoding dict built by build_word_encoding()\n",
        "    :param data: list of strings\n",
        "    :param vector_size: size of each encoded vector\n",
        "    \"\"\"\n",
        "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "    for i in range(len(data)):\n",
        "        for j in range(min(len(data[i]), vector_size)):\n",
        "            try:\n",
        "                transformed_data[i][j] = encoding[data[i][j]]\n",
        "            except:\n",
        "                transformed_data[i][j] = encoding['<UNK>']\n",
        "    return transformed_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZMkqnOMXOq7",
        "colab_type": "code",
        "outputId": "4420afed-075b-4a0d-8966-1412e3538ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#encoding training set\n",
        "encoded_training_input = transform(\n",
        "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
        "encoded_training_output = transform(\n",
        "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_training_input', encoded_training_input.shape)\n",
        "print('encoded_training_output', encoded_training_output.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_training_input (3086, 20)\n",
            "encoded_training_output (3086, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6_71QT7XR0n",
        "colab_type": "code",
        "outputId": "5b761468-9530-492e-c27b-bb5129a9b516",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#encoding validation set\n",
        "encoded_validation_input = transform(\n",
        "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "encoded_validation_output = transform(\n",
        "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_validation_input', encoded_validation_input.shape)\n",
        "print('encoded_validation_output', encoded_validation_output.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "encoded_validation_input (772, 20)\n",
            "encoded_validation_output (772, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzrh-6LXXWdh",
        "colab_type": "text"
      },
      "source": [
        "# 2 Model Building\n",
        "2.1 Sequence-to-Sequence in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_X8meUHXctV",
        "colab_type": "code",
        "outputId": "1c5bfa2d-82d6-4205-b936-9af252ff22b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20\n",
        "\n",
        "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
        "decoder_input = Input(shape=(OUTPUT_LENGTH,))\n",
        "\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
        "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "print('encoder', encoder)\n",
        "print('encoder_last', encoder_last)\n",
        "\n",
        "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
        "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
        "\n",
        "print('decoder', decoder)\n",
        "\n",
        "# For the plain Sequence-to-Sequence, we produced the output from directly from decoder\n",
        "# output = TimeDistributed(Dense(output_dict_size, activation=\"softmax\"))(decoder)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3156: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "encoder Tensor(\"lstm_1/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n",
            "encoder_last Tensor(\"strided_slice:0\", shape=(?, 512), dtype=float32)\n",
            "decoder Tensor(\"lstm_2/transpose_2:0\", shape=(?, 20, 512), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY6lS-FCXp5Q",
        "colab_type": "text"
      },
      "source": [
        "2.2 Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ea9G-aLXqk2",
        "colab_type": "code",
        "outputId": "86fba9a4-5b9b-476d-ef24-88dcf39b52cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from keras.layers import Activation, dot, concatenate\n",
        "\n",
        "# Equation (7) with 'dot' score from Section 3.1 in the paper.\n",
        "# Note that we reuse Softmax-activation layer instead of writing tensor calculation\n",
        "attention = dot([decoder, encoder], axes=[2, 2])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "print('attention', attention)\n",
        "\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "print('context', context)\n",
        "\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "print('decoder_combined_context', decoder_combined_context)\n",
        "\n",
        "# Has another weight + tanh layer as described in equation (5) of the paper\n",
        "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
        "print('output', output)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention Tensor(\"attention/truediv:0\", shape=(?, 20, 20), dtype=float32)\n",
            "context Tensor(\"dot_2/MatMul:0\", shape=(?, 20, 512), dtype=float32)\n",
            "decoder_combined_context Tensor(\"concatenate_1/concat:0\", shape=(?, 20, 1024), dtype=float32)\n",
            "output Tensor(\"time_distributed_2/Reshape_1:0\", shape=(?, 20, 1219), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAxqFq62XvXi",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569rgrqXXyNj",
        "colab_type": "code",
        "outputId": "f1077cd2-014d-46f0-f721-b3b8fba18a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')#, metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3622: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            (None, 20)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 20, 128)      156032      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 128)      156032      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   (None, 20, 512)      1312768     embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 20, 20)       0           lstm_2[0][0]                     \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 20, 20)       0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dot_2 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 20, 1024)     0           dot_2[0][0]                      \n",
            "                                                                 lstm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 512)      524800      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 20, 1219)     625347      time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 4,087,747\n",
            "Trainable params: 4,087,747\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTKCcux4X4yR",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfMY61sYX492",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_encoder_input = encoded_training_input\n",
        "training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
        "training_decoder_input[:, 0] = WORD_CODE_START\n",
        "training_decoder_output = encoded_training_output.reshape(training_size, INPUT_LENGTH, 1)\n",
        "#training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
        "\n",
        "validation_encoder_input = encoded_validation_input\n",
        "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
        "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
        "validation_decoder_input[:, 0] = WORD_CODE_START\n",
        "validation_decoder_output = encoded_validation_output.reshape(validation_size, OUTPUT_LENGTH, 1)\n",
        "#validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCcOgwU2YoSB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmm9zT9JYoa-",
        "colab_type": "code",
        "outputId": "bac4cc37-22ce-4948-9417-f4717887a5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],    \n",
        "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
        "          #validation_split=0.05,\n",
        "          batch_size=128, epochs=100)\n",
        "\n",
        "model.save(base_path + 'model_attention.h5')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 3086 samples, validate on 772 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "3086/3086 [==============================] - 18s 6ms/step - loss: 6.0351 - val_loss: 5.4523\n",
            "Epoch 2/100\n",
            "3086/3086 [==============================] - 1s 406us/step - loss: 5.2812 - val_loss: 5.2643\n",
            "Epoch 3/100\n",
            "3086/3086 [==============================] - 1s 403us/step - loss: 5.0078 - val_loss: 5.0920\n",
            "Epoch 4/100\n",
            "3086/3086 [==============================] - 1s 412us/step - loss: 4.7722 - val_loss: 4.9269\n",
            "Epoch 5/100\n",
            "3086/3086 [==============================] - 1s 414us/step - loss: 4.5702 - val_loss: 4.7690\n",
            "Epoch 6/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 4.3745 - val_loss: 4.6341\n",
            "Epoch 7/100\n",
            "3086/3086 [==============================] - 1s 413us/step - loss: 4.1923 - val_loss: 4.5242\n",
            "Epoch 8/100\n",
            "3086/3086 [==============================] - 1s 395us/step - loss: 4.0280 - val_loss: 4.4534\n",
            "Epoch 9/100\n",
            "3086/3086 [==============================] - 1s 397us/step - loss: 3.8839 - val_loss: 4.4181\n",
            "Epoch 10/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 3.7638 - val_loss: 4.3468\n",
            "Epoch 11/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 3.6454 - val_loss: 4.3207\n",
            "Epoch 12/100\n",
            "3086/3086 [==============================] - 1s 415us/step - loss: 3.5386 - val_loss: 4.2903\n",
            "Epoch 13/100\n",
            "3086/3086 [==============================] - 1s 409us/step - loss: 3.4446 - val_loss: 4.2464\n",
            "Epoch 14/100\n",
            "3086/3086 [==============================] - 1s 397us/step - loss: 3.3411 - val_loss: 4.2472\n",
            "Epoch 15/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 3.2530 - val_loss: 4.2093\n",
            "Epoch 16/100\n",
            "3086/3086 [==============================] - 1s 413us/step - loss: 3.1337 - val_loss: 4.2169\n",
            "Epoch 17/100\n",
            "3086/3086 [==============================] - 1s 413us/step - loss: 3.0322 - val_loss: 4.1870\n",
            "Epoch 18/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 2.9361 - val_loss: 4.1931\n",
            "Epoch 19/100\n",
            "3086/3086 [==============================] - 1s 412us/step - loss: 2.8321 - val_loss: 4.1954\n",
            "Epoch 20/100\n",
            "3086/3086 [==============================] - 1s 413us/step - loss: 2.7254 - val_loss: 4.1770\n",
            "Epoch 21/100\n",
            "3086/3086 [==============================] - 1s 435us/step - loss: 2.6191 - val_loss: 4.1660\n",
            "Epoch 22/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 2.5156 - val_loss: 4.1807\n",
            "Epoch 23/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 2.4302 - val_loss: 4.1697\n",
            "Epoch 24/100\n",
            "3086/3086 [==============================] - 1s 397us/step - loss: 2.3057 - val_loss: 4.1838\n",
            "Epoch 25/100\n",
            "3086/3086 [==============================] - 1s 403us/step - loss: 2.2055 - val_loss: 4.1957\n",
            "Epoch 26/100\n",
            "3086/3086 [==============================] - 1s 407us/step - loss: 2.1063 - val_loss: 4.1910\n",
            "Epoch 27/100\n",
            "3086/3086 [==============================] - 1s 395us/step - loss: 1.9724 - val_loss: 4.2333\n",
            "Epoch 28/100\n",
            "3086/3086 [==============================] - 1s 408us/step - loss: 1.8668 - val_loss: 4.2307\n",
            "Epoch 29/100\n",
            "3086/3086 [==============================] - 1s 414us/step - loss: 1.7570 - val_loss: 4.2573\n",
            "Epoch 30/100\n",
            "3086/3086 [==============================] - 1s 417us/step - loss: 1.6475 - val_loss: 4.2844\n",
            "Epoch 31/100\n",
            "3086/3086 [==============================] - 1s 394us/step - loss: 1.5566 - val_loss: 4.3172\n",
            "Epoch 32/100\n",
            "3086/3086 [==============================] - 1s 411us/step - loss: 1.4772 - val_loss: 4.3400\n",
            "Epoch 33/100\n",
            "3086/3086 [==============================] - 1s 414us/step - loss: 1.3668 - val_loss: 4.3649\n",
            "Epoch 34/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 1.2733 - val_loss: 4.4101\n",
            "Epoch 35/100\n",
            "3086/3086 [==============================] - 1s 400us/step - loss: 1.1960 - val_loss: 4.4415\n",
            "Epoch 36/100\n",
            "3086/3086 [==============================] - 1s 394us/step - loss: 1.1022 - val_loss: 4.4808\n",
            "Epoch 37/100\n",
            "3086/3086 [==============================] - 1s 399us/step - loss: 1.0166 - val_loss: 4.5077\n",
            "Epoch 38/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 0.9594 - val_loss: 4.5302\n",
            "Epoch 39/100\n",
            "3086/3086 [==============================] - 1s 390us/step - loss: 0.8730 - val_loss: 4.5671\n",
            "Epoch 40/100\n",
            "3086/3086 [==============================] - 1s 399us/step - loss: 0.8156 - val_loss: 4.6233\n",
            "Epoch 41/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 0.7413 - val_loss: 4.6486\n",
            "Epoch 42/100\n",
            "3086/3086 [==============================] - 1s 403us/step - loss: 0.6713 - val_loss: 4.6995\n",
            "Epoch 43/100\n",
            "3086/3086 [==============================] - 1s 429us/step - loss: 0.6081 - val_loss: 4.7380\n",
            "Epoch 44/100\n",
            "3086/3086 [==============================] - 1s 428us/step - loss: 0.5513 - val_loss: 4.7682\n",
            "Epoch 45/100\n",
            "3086/3086 [==============================] - 1s 411us/step - loss: 0.4971 - val_loss: 4.8140\n",
            "Epoch 46/100\n",
            "3086/3086 [==============================] - 1s 403us/step - loss: 0.4464 - val_loss: 4.8419\n",
            "Epoch 47/100\n",
            "3086/3086 [==============================] - 1s 398us/step - loss: 0.4023 - val_loss: 4.8988\n",
            "Epoch 48/100\n",
            "3086/3086 [==============================] - 1s 397us/step - loss: 0.3636 - val_loss: 4.9637\n",
            "Epoch 49/100\n",
            "3086/3086 [==============================] - 1s 402us/step - loss: 0.3323 - val_loss: 4.9834\n",
            "Epoch 50/100\n",
            "3086/3086 [==============================] - 1s 410us/step - loss: 0.3085 - val_loss: 5.0253\n",
            "Epoch 51/100\n",
            "3086/3086 [==============================] - 1s 402us/step - loss: 0.2751 - val_loss: 5.0423\n",
            "Epoch 52/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 0.2401 - val_loss: 5.0933\n",
            "Epoch 53/100\n",
            "3086/3086 [==============================] - 1s 399us/step - loss: 0.2181 - val_loss: 5.1349\n",
            "Epoch 54/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 0.1955 - val_loss: 5.1742\n",
            "Epoch 55/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 0.1747 - val_loss: 5.2204\n",
            "Epoch 56/100\n",
            "3086/3086 [==============================] - 1s 393us/step - loss: 0.1499 - val_loss: 5.2483\n",
            "Epoch 57/100\n",
            "3086/3086 [==============================] - 1s 398us/step - loss: 0.1361 - val_loss: 5.2737\n",
            "Epoch 58/100\n",
            "3086/3086 [==============================] - 1s 392us/step - loss: 0.1278 - val_loss: 5.3240\n",
            "Epoch 59/100\n",
            "3086/3086 [==============================] - 1s 390us/step - loss: 0.1189 - val_loss: 5.3459\n",
            "Epoch 60/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 0.1074 - val_loss: 5.3638\n",
            "Epoch 61/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 0.0991 - val_loss: 5.3868\n",
            "Epoch 62/100\n",
            "3086/3086 [==============================] - 1s 402us/step - loss: 0.0907 - val_loss: 5.4103\n",
            "Epoch 63/100\n",
            "3086/3086 [==============================] - 1s 392us/step - loss: 0.0801 - val_loss: 5.4562\n",
            "Epoch 64/100\n",
            "3086/3086 [==============================] - 1s 408us/step - loss: 0.0736 - val_loss: 5.4783\n",
            "Epoch 65/100\n",
            "3086/3086 [==============================] - 1s 401us/step - loss: 0.0676 - val_loss: 5.5148\n",
            "Epoch 66/100\n",
            "3086/3086 [==============================] - 1s 406us/step - loss: 0.0634 - val_loss: 5.5300\n",
            "Epoch 67/100\n",
            "3086/3086 [==============================] - 1s 401us/step - loss: 0.0586 - val_loss: 5.5729\n",
            "Epoch 68/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 0.0584 - val_loss: 5.5825\n",
            "Epoch 69/100\n",
            "3086/3086 [==============================] - 1s 434us/step - loss: 0.0570 - val_loss: 5.5970\n",
            "Epoch 70/100\n",
            "3086/3086 [==============================] - 1s 398us/step - loss: 0.0556 - val_loss: 5.6321\n",
            "Epoch 71/100\n",
            "3086/3086 [==============================] - 1s 400us/step - loss: 0.0510 - val_loss: 5.6530\n",
            "Epoch 72/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 0.0513 - val_loss: 5.6735\n",
            "Epoch 73/100\n",
            "3086/3086 [==============================] - 1s 412us/step - loss: 0.0519 - val_loss: 5.6885\n",
            "Epoch 74/100\n",
            "3086/3086 [==============================] - 1s 406us/step - loss: 0.0542 - val_loss: 5.6900\n",
            "Epoch 75/100\n",
            "3086/3086 [==============================] - 1s 399us/step - loss: 0.0555 - val_loss: 5.7049\n",
            "Epoch 76/100\n",
            "3086/3086 [==============================] - 1s 402us/step - loss: 0.0559 - val_loss: 5.7075\n",
            "Epoch 77/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 0.0599 - val_loss: 5.7284\n",
            "Epoch 78/100\n",
            "3086/3086 [==============================] - 1s 415us/step - loss: 0.0803 - val_loss: 5.7246\n",
            "Epoch 79/100\n",
            "3086/3086 [==============================] - 1s 405us/step - loss: 0.1446 - val_loss: 5.7061\n",
            "Epoch 80/100\n",
            "3086/3086 [==============================] - 1s 393us/step - loss: 0.1850 - val_loss: 5.6130\n",
            "Epoch 81/100\n",
            "3086/3086 [==============================] - 1s 396us/step - loss: 0.2140 - val_loss: 5.6031\n",
            "Epoch 82/100\n",
            "3086/3086 [==============================] - 1s 402us/step - loss: 0.1570 - val_loss: 5.5791\n",
            "Epoch 83/100\n",
            "3086/3086 [==============================] - 1s 400us/step - loss: 0.1009 - val_loss: 5.5949\n",
            "Epoch 84/100\n",
            "3086/3086 [==============================] - 1s 398us/step - loss: 0.0676 - val_loss: 5.6319\n",
            "Epoch 85/100\n",
            "3086/3086 [==============================] - 1s 401us/step - loss: 0.0515 - val_loss: 5.6446\n",
            "Epoch 86/100\n",
            "3086/3086 [==============================] - 1s 399us/step - loss: 0.0460 - val_loss: 5.6764\n",
            "Epoch 87/100\n",
            "3086/3086 [==============================] - 1s 419us/step - loss: 0.0393 - val_loss: 5.6987\n",
            "Epoch 88/100\n",
            "3086/3086 [==============================] - 1s 407us/step - loss: 0.0363 - val_loss: 5.7212\n",
            "Epoch 89/100\n",
            "3086/3086 [==============================] - 1s 403us/step - loss: 0.0348 - val_loss: 5.7331\n",
            "Epoch 90/100\n",
            "3086/3086 [==============================] - 1s 393us/step - loss: 0.0333 - val_loss: 5.7539\n",
            "Epoch 91/100\n",
            "3086/3086 [==============================] - 1s 412us/step - loss: 0.0331 - val_loss: 5.7622\n",
            "Epoch 92/100\n",
            "3086/3086 [==============================] - 1s 400us/step - loss: 0.0332 - val_loss: 5.7812\n",
            "Epoch 93/100\n",
            "3086/3086 [==============================] - 1s 430us/step - loss: 0.0344 - val_loss: 5.8035\n",
            "Epoch 94/100\n",
            "3086/3086 [==============================] - 1s 391us/step - loss: 0.0320 - val_loss: 5.8242\n",
            "Epoch 95/100\n",
            "3086/3086 [==============================] - 1s 390us/step - loss: 0.0301 - val_loss: 5.8298\n",
            "Epoch 96/100\n",
            "3086/3086 [==============================] - 1s 404us/step - loss: 0.0295 - val_loss: 5.8350\n",
            "Epoch 97/100\n",
            "3086/3086 [==============================] - 1s 408us/step - loss: 0.0296 - val_loss: 5.8459\n",
            "Epoch 98/100\n",
            "3086/3086 [==============================] - 1s 401us/step - loss: 0.0301 - val_loss: 5.8620\n",
            "Epoch 99/100\n",
            "3086/3086 [==============================] - 1s 407us/step - loss: 0.0290 - val_loss: 5.8743\n",
            "Epoch 100/100\n",
            "3086/3086 [==============================] - 1s 401us/step - loss: 0.0281 - val_loss: 5.8844\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:883: UserWarning: Layer lstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'strided_slice:0' shape=(?, 512) dtype=float32>, <tf.Tensor 'strided_slice:0' shape=(?, 512) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
            "  '. They will not be included '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks07KcG-1Ku_",
        "colab_type": "text"
      },
      "source": [
        "# 3. Model testing¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETGXPSv_0-mi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prediction(raw_input):\n",
        "    clean_input = raw_input\n",
        "    input_tok = [nltk.word_tokenize(clean_input)]\n",
        "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
        "    encoder_input = transform(encoding, input_tok, 20)\n",
        "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "    decoder_input[:,0] = WORD_CODE_START\n",
        "    for i in range(1, OUTPUT_LENGTH):\n",
        "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
        "        decoder_input[:,i] = output[:,i]\n",
        "    return output\n",
        "\n",
        "def decode(decoding, vector):\n",
        "    \"\"\"\n",
        "    :param decoding: decoding dict built by word encoding\n",
        "    :param vector: an encoded vector\n",
        "    \"\"\"\n",
        "    text = ''\n",
        "    for i in vector:\n",
        "        if i == 0:\n",
        "            break\n",
        "        text += ' '\n",
        "        text += decoding[i]\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4XOJXBnspQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rXKR0oe1JlI",
        "colab_type": "code",
        "outputId": "1352f062-d88c-476c-e958-a9aab7f8317c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "    questions = 'bạn biết nói tiếng anh không?'\n",
        "    output = prediction(questions)\n",
        "    print ('Q:', questions)\n",
        "    print ('A:', decode(decoding, output[0]))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: bạn biết nói tiếng anh không?\n",
            "A:  Vâng , tôi được một <UNK> đang hay . Bạn nghĩ thế nào ?\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}